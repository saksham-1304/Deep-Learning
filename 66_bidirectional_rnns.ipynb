{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59f1e160",
   "metadata": {},
   "source": [
    "## 1. Introduction & Motivation\n",
    "\n",
    "### What We've Learned So Far\n",
    "\n",
    "In our RNN journey, we have covered:\n",
    "1. **Vanilla RNN** - Basic recurrent neural network\n",
    "2. **LSTM** - Long Short-Term Memory (handles long-term dependencies)\n",
    "3. **GRU** - Gated Recurrent Unit (simpler alternative to LSTM)\n",
    "4. **Deep RNNs** - Stacking multiple RNN layers\n",
    "\n",
    "Now, we'll learn one final important concept: **Bidirectional RNNs**\n",
    "\n",
    "### The Limitation of Unidirectional RNNs\n",
    "\n",
    "In a standard (unidirectional) RNN:\n",
    "\n",
    "```\n",
    "Time Step 1        Time Step 2        Time Step 3\n",
    "    ‚Üì                  ‚Üì                  ‚Üì\n",
    "  [x‚ÇÅ]              [x‚ÇÇ]              [x‚ÇÉ]\n",
    "    ‚Üì                  ‚Üì                  ‚Üì\n",
    "  ‚îå‚îÄ‚îÄ‚îÄ‚îê    h‚ÇÅ      ‚îå‚îÄ‚îÄ‚îÄ‚îê    h‚ÇÇ      ‚îå‚îÄ‚îÄ‚îÄ‚îê    h‚ÇÉ\n",
    "  ‚îÇRNN‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí  ‚îÇRNN‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí  ‚îÇRNN‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí ≈∑\n",
    "  ‚îî‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Key Observation:**\n",
    "- Information flows **only from left to right**\n",
    "- The final output ≈∑ depends on x‚ÇÅ, x‚ÇÇ, x‚ÇÉ (all **past** inputs)\n",
    "- But what if **future inputs** should affect **past outputs**?\n",
    "\n",
    "### When Future Inputs Affect Past Outputs\n",
    "\n",
    "There are many NLP scenarios where understanding the **full context** (both past AND future) is crucial:\n",
    "\n",
    "| Scenario | Why Future Context Matters |\n",
    "|----------|---------------------------|\n",
    "| Named Entity Recognition | \"Amazon\" could be a company OR a river - need to see following words |\n",
    "| Machine Translation | Word order differs between languages |\n",
    "| Part-of-Speech Tagging | Same word can be noun/verb depending on context |\n",
    "| Sentiment Analysis | Negation words can flip meaning of earlier words |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d3dbaa",
   "metadata": {},
   "source": [
    "## 2. The Problem: Named Entity Recognition (NER) Example\n",
    "\n",
    "### What is NER?\n",
    "\n",
    "**Named Entity Recognition (NER)** is an NLP task where you identify and classify named entities in text:\n",
    "\n",
    "- **Person (PER)**: \"Nitish\", \"Elon Musk\"\n",
    "- **Organization (ORG)**: \"Google\", \"Amazon\"\n",
    "- **Location (LOC)**: \"Delhi\", \"Amazon River\"\n",
    "\n",
    "### The Amazon Problem\n",
    "\n",
    "Consider these two sentences:\n",
    "\n",
    "**Sentence 1:** \"I love **Amazon**. It's a great **website**.\"\n",
    "\n",
    "**Sentence 2:** \"I love **Amazon**. It's a beautiful **river**.\"\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "```\n",
    "Processing left-to-right:\n",
    "\n",
    "\"I\" ‚Üí \"love\" ‚Üí \"Amazon\" ‚Üí ???\n",
    "                   ‚Üì\n",
    "          Is it ORG or LOC?\n",
    "          We don't know yet!\n",
    "```\n",
    "\n",
    "When processing \"Amazon\":\n",
    "- If we only see \"I love Amazon\" ‚Üí **Ambiguous!** (Could be company or river)\n",
    "- If we also see \"...website\" ‚Üí **Organization (ORG)**\n",
    "- If we also see \"...river\" ‚Üí **Location (LOC)**\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "**The future input (\"website\" or \"river\") affects the classification of a past word (\"Amazon\")!**\n",
    "\n",
    "A unidirectional RNN cannot handle this because it only has access to past context, not future context.\n",
    "\n",
    "**Solution: Bidirectional RNN** - Process the sequence from **both directions**!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a960753b",
   "metadata": {},
   "source": [
    "## 3. Bidirectional RNN Architecture\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "Bidirectional RNN uses **two separate RNNs**:\n",
    "1. **Forward RNN** (‚Üí): Processes sequence left-to-right\n",
    "2. **Backward RNN** (‚Üê): Processes sequence right-to-left\n",
    "\n",
    "Then, at each time step, we **concatenate** the outputs from both RNNs.\n",
    "\n",
    "### Architecture Diagram\n",
    "\n",
    "```\n",
    "                  x‚ÇÅ              x‚ÇÇ              x‚ÇÉ              x‚ÇÑ\n",
    "               (Amazon)         (the)          (best)        (website)\n",
    "                  ‚Üì               ‚Üì               ‚Üì               ‚Üì\n",
    "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "Forward ‚Üí     ‚îÇ  RNN  ‚îÇ ‚îÄ‚îÄ‚îÄ‚Üí  ‚îÇ  RNN  ‚îÇ ‚îÄ‚îÄ‚îÄ‚Üí  ‚îÇ  RNN  ‚îÇ ‚îÄ‚îÄ‚îÄ‚Üí  ‚îÇ  RNN  ‚îÇ\n",
    "  (Blue)      ‚îÇ   ‚Üí   ‚îÇ       ‚îÇ   ‚Üí   ‚îÇ       ‚îÇ   ‚Üí   ‚îÇ       ‚îÇ   ‚Üí   ‚îÇ\n",
    "              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                  ‚Üì               ‚Üì               ‚Üì               ‚Üì\n",
    "                 h‚ÇÅ‚Üí             h‚ÇÇ‚Üí             h‚ÇÉ‚Üí             h‚ÇÑ‚Üí\n",
    "                  ‚Üì               ‚Üì               ‚Üì               ‚Üì\n",
    "              [CONCAT]        [CONCAT]        [CONCAT]        [CONCAT]\n",
    "                  ‚Üë               ‚Üë               ‚Üë               ‚Üë\n",
    "                 h‚ÇÅ‚Üê             h‚ÇÇ‚Üê             h‚ÇÉ‚Üê             h‚ÇÑ‚Üê\n",
    "                  ‚Üë               ‚Üë               ‚Üë               ‚Üë\n",
    "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "Backward ‚Üê    ‚îÇ  RNN  ‚îÇ ‚Üê‚îÄ‚îÄ‚îÄ  ‚îÇ  RNN  ‚îÇ ‚Üê‚îÄ‚îÄ‚îÄ  ‚îÇ  RNN  ‚îÇ ‚Üê‚îÄ‚îÄ‚îÄ  ‚îÇ  RNN  ‚îÇ\n",
    "  (Green)     ‚îÇ   ‚Üê   ‚îÇ       ‚îÇ   ‚Üê   ‚îÇ       ‚îÇ   ‚Üê   ‚îÇ       ‚îÇ   ‚Üê   ‚îÇ\n",
    "              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                  ‚Üë               ‚Üë               ‚Üë               ‚Üë\n",
    "               (Amazon)         (the)          (best)        (website)\n",
    "                  ‚Üì               ‚Üì               ‚Üì               ‚Üì\n",
    "                 ≈∑‚ÇÅ              ≈∑‚ÇÇ              ≈∑‚ÇÉ              ≈∑‚ÇÑ\n",
    "```\n",
    "\n",
    "### How It Solves the Amazon Problem\n",
    "\n",
    "At time step 1 (processing \"Amazon\"):\n",
    "\n",
    "- **Forward RNN (h‚ÇÅ‚Üí)**: Has seen only \"Amazon\"\n",
    "- **Backward RNN (h‚ÇÅ‚Üê)**: Has seen \"website\", \"best\", \"the\", \"Amazon\"\n",
    "\n",
    "When we concatenate [h‚ÇÅ‚Üí, h‚ÇÅ‚Üê], the output ≈∑‚ÇÅ has information from **both directions**!\n",
    "\n",
    "Now the model knows \"website\" comes later ‚Üí \"Amazon\" is an **Organization**!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee38c881",
   "metadata": {},
   "source": [
    "## 4. Mathematical Formulation\n",
    "\n",
    "### Forward RNN Equation\n",
    "\n",
    "The forward hidden state at time $t$:\n",
    "\n",
    "$$\\overrightarrow{h_t} = \\tanh(W_{\\rightarrow} \\cdot \\overrightarrow{h_{t-1}} + U \\cdot x_t + b_{\\rightarrow})$$\n",
    "\n",
    "Where:\n",
    "- $\\overrightarrow{h_t}$ = Forward hidden state at time $t$\n",
    "- $\\overrightarrow{h_{t-1}}$ = Previous forward hidden state\n",
    "- $W_{\\rightarrow}$ = Forward recurrent weights\n",
    "- $U$ = Input weights\n",
    "- $b_{\\rightarrow}$ = Forward bias\n",
    "\n",
    "### Backward RNN Equation\n",
    "\n",
    "The backward hidden state at time $t$:\n",
    "\n",
    "$$\\overleftarrow{h_t} = \\tanh(W_{\\leftarrow} \\cdot \\overleftarrow{h_{t+1}} + U \\cdot x_t + b_{\\leftarrow})$$\n",
    "\n",
    "**Key Difference:** Notice $h_{t+1}$ instead of $h_{t-1}$!\n",
    "- Time step 4 affects time step 3\n",
    "- Time step 3 affects time step 2\n",
    "- And so on...\n",
    "\n",
    "### Output Equation\n",
    "\n",
    "The final output at time $t$ is computed by concatenating both hidden states:\n",
    "\n",
    "$$\\hat{y}_t = \\sigma(W_y \\cdot [\\overrightarrow{h_t}; \\overleftarrow{h_t}] + b_y)$$\n",
    "\n",
    "Where:\n",
    "- $[\\overrightarrow{h_t}; \\overleftarrow{h_t}]$ = Concatenation of forward and backward hidden states\n",
    "- $W_y$ = Output weights\n",
    "- $b_y$ = Output bias\n",
    "- $\\sigma$ = Activation function (sigmoid for binary, softmax for multi-class)\n",
    "\n",
    "### Summary of Equations\n",
    "\n",
    "| Component | Equation |\n",
    "|-----------|----------|\n",
    "| Forward Hidden | $\\overrightarrow{h_t} = \\tanh(W_{\\rightarrow} \\cdot \\overrightarrow{h_{t-1}} + U \\cdot x_t + b_{\\rightarrow})$ |\n",
    "| Backward Hidden | $\\overleftarrow{h_t} = \\tanh(W_{\\leftarrow} \\cdot \\overleftarrow{h_{t+1}} + U \\cdot x_t + b_{\\leftarrow})$ |\n",
    "| Output | $\\hat{y}_t = \\sigma(W_y \\cdot [\\overrightarrow{h_t}; \\overleftarrow{h_t}] + b_y)$ |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaef0f03",
   "metadata": {},
   "source": [
    "## 5. Implementation in Keras\n",
    "\n",
    "Keras provides a `Bidirectional` wrapper that makes it extremely easy to create bidirectional RNNs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1126d87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense, Bidirectional\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243a9680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDB dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "\n",
    "# Pad sequences\n",
    "x_train = pad_sequences(x_train, maxlen=100)\n",
    "x_test = pad_sequences(x_test, maxlen=100)\n",
    "\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ca3dd2",
   "metadata": {},
   "source": [
    "### 5.1 Unidirectional SimpleRNN (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f50b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unidirectional SimpleRNN\n",
    "unidirectional_model = Sequential([\n",
    "    Embedding(10000, 32, input_length=100),\n",
    "    SimpleRNN(5),  # 5 units\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "unidirectional_model.build(input_shape=(None, 100))\n",
    "print(\"Unidirectional SimpleRNN:\")\n",
    "unidirectional_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b02d0a9",
   "metadata": {},
   "source": [
    "### 5.2 Bidirectional SimpleRNN\n",
    "\n",
    "Converting to bidirectional is simple - just wrap the RNN layer with `Bidirectional()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7291f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional SimpleRNN\n",
    "bidirectional_rnn = Sequential([\n",
    "    Embedding(10000, 32, input_length=100),\n",
    "    Bidirectional(SimpleRNN(5)),  # Wrapped with Bidirectional!\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "bidirectional_rnn.build(input_shape=(None, 100))\n",
    "print(\"Bidirectional SimpleRNN:\")\n",
    "bidirectional_rnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5f5ec8",
   "metadata": {},
   "source": [
    "### Understanding the Parameter Count\n",
    "\n",
    "Notice:\n",
    "- **Unidirectional SimpleRNN**: 190 parameters\n",
    "- **Bidirectional SimpleRNN**: 380 parameters (exactly **2x**!)\n",
    "\n",
    "This is because bidirectional uses **two separate RNNs** (forward + backward), each with its own weights!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ed6642",
   "metadata": {},
   "source": [
    "### 5.3 Bidirectional LSTM (BiLSTM)\n",
    "\n",
    "The same bidirectional concept works with LSTM and GRU. **BiLSTM** is very commonly used in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e3783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional LSTM (BiLSTM)\n",
    "bilstm_model = Sequential([\n",
    "    Embedding(10000, 32, input_length=100),\n",
    "    Bidirectional(LSTM(5)),  # BiLSTM!\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "bilstm_model.build(input_shape=(None, 100))\n",
    "print(\"Bidirectional LSTM (BiLSTM):\")\n",
    "bilstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6dbc94",
   "metadata": {},
   "source": [
    "### 5.4 Bidirectional GRU (BiGRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a0c05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional GRU (BiGRU)\n",
    "bigru_model = Sequential([\n",
    "    Embedding(10000, 32, input_length=100),\n",
    "    Bidirectional(GRU(5)),  # BiGRU!\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "bigru_model.build(input_shape=(None, 100))\n",
    "print(\"Bidirectional GRU (BiGRU):\")\n",
    "bigru_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95307de6",
   "metadata": {},
   "source": [
    "### Parameter Comparison\n",
    "\n",
    "Let's compare the parameter counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450995e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter comparison\n",
    "models = {\n",
    "    'Unidirectional RNN': unidirectional_model,\n",
    "    'Bidirectional RNN': bidirectional_rnn,\n",
    "    'BiLSTM': bilstm_model,\n",
    "    'BiGRU': bigru_model\n",
    "}\n",
    "\n",
    "print(\"Parameter Count Comparison:\")\n",
    "print(\"=\" * 45)\n",
    "for name, model in models.items():\n",
    "    # Get only RNN layer params (excluding embedding and dense)\n",
    "    rnn_params = model.layers[1].count_params()\n",
    "    total_params = model.count_params()\n",
    "    print(f\"{name:25s}: RNN Layer = {rnn_params:5d}, Total = {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308c9320",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Practical Example: Sentiment Analysis with BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638e8d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a BiLSTM model for sentiment analysis\n",
    "model = Sequential([\n",
    "    Embedding(10000, 32, input_length=100),\n",
    "    Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18d1e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ff92d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "axes[0].set_title('BiLSTM: Training vs Validation Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "axes[1].set_title('BiLSTM: Training vs Validation Accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6792686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c5069e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Stacked Bidirectional RNNs\n",
    "\n",
    "You can also stack multiple bidirectional layers (Deep Bidirectional RNN)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5fc35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked Bidirectional LSTM\n",
    "stacked_bilstm = Sequential([\n",
    "    Embedding(10000, 32, input_length=100),\n",
    "    Bidirectional(LSTM(32, return_sequences=True)),  # First BiLSTM layer\n",
    "    Bidirectional(LSTM(16)),                          # Second BiLSTM layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "stacked_bilstm.build(input_shape=(None, 100))\n",
    "print(\"Stacked Bidirectional LSTM:\")\n",
    "stacked_bilstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bbda95",
   "metadata": {},
   "source": [
    "### Understanding Output Shapes\n",
    "\n",
    "- **First BiLSTM (32 units)**: Output shape = `(None, 100, 64)`\n",
    "  - 32 forward + 32 backward = **64 features per time step**\n",
    "  - `return_sequences=True` keeps all 100 time steps\n",
    "\n",
    "- **Second BiLSTM (16 units)**: Output shape = `(None, 32)`\n",
    "  - 16 forward + 16 backward = **32 features**\n",
    "  - Only returns final state (for classification)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808793aa",
   "metadata": {},
   "source": [
    "## 8. Applications of Bidirectional RNNs\n",
    "\n",
    "### Best Use Cases\n",
    "\n",
    "| Application | Why Bidirectional Helps |\n",
    "|-------------|------------------------|\n",
    "| **Named Entity Recognition (NER)** | Context from both sides determines entity type |\n",
    "| **Part-of-Speech (POS) Tagging** | Same word can be different POS based on context |\n",
    "| **Machine Translation** | Word order differs between languages |\n",
    "| **Sentiment Analysis** | Negations and modifiers can appear before or after |\n",
    "| **Time Series Forecasting** | Historical patterns in both directions help |\n",
    "\n",
    "### Real-World Examples\n",
    "\n",
    "1. **NER**: \"Apple announced...\" vs \"I ate an apple...\" ‚Üí Company vs Fruit\n",
    "2. **POS Tagging**: \"I can fish\" (verb) vs \"I caught a fish\" (noun)\n",
    "3. **Sentiment**: \"not bad\" ‚Üí positive despite \"not\" and \"bad\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc05d7a",
   "metadata": {},
   "source": [
    "## 9. Advantages and Disadvantages\n",
    "\n",
    "### ‚úÖ Advantages\n",
    "\n",
    "| Advantage | Description |\n",
    "|-----------|-------------|\n",
    "| **Full Context** | Access to both past and future information at every time step |\n",
    "| **Better Accuracy** | Often outperforms unidirectional RNNs on many NLP tasks |\n",
    "| **Flexible** | Works with any RNN cell (SimpleRNN, LSTM, GRU) |\n",
    "| **Easy to Implement** | Keras `Bidirectional()` wrapper makes it simple |\n",
    "\n",
    "### ‚ùå Disadvantages\n",
    "\n",
    "| Disadvantage | Description |\n",
    "|--------------|-------------|\n",
    "| **Double Parameters** | 2x weights and biases ‚Üí Increased training time |\n",
    "| **Overfitting Risk** | More parameters = higher chance of overfitting |\n",
    "| **Latency Issues** | Cannot be used in **real-time** applications |\n",
    "| **Requires Full Sequence** | Need entire input before processing |\n",
    "\n",
    "### When NOT to Use Bidirectional RNNs\n",
    "\n",
    "**Real-time Speech Recognition Example:**\n",
    "\n",
    "```\n",
    "User speaking: \"Hi my name is...\"\n",
    "                    ‚Üì\n",
    "           BiRNN: \"I need to wait for\n",
    "                   the complete sentence!\"\n",
    "                    ‚Üì\n",
    "           High latency! ‚ùå\n",
    "```\n",
    "\n",
    "For real-time applications where data arrives incrementally, unidirectional RNNs are preferred.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d33315",
   "metadata": {},
   "source": [
    "## 10. Visualization: Information Flow Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395cd3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Unidirectional RNN\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 6)\n",
    "ax1.set_title('Unidirectional RNN', fontsize=14, fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Draw boxes and arrows for unidirectional\n",
    "for i, (x, word) in enumerate([(1, 'x‚ÇÅ'), (3.5, 'x‚ÇÇ'), (6, 'x‚ÇÉ'), (8.5, 'x‚ÇÑ')]):\n",
    "    # Input\n",
    "    ax1.text(x+0.5, 1, word, ha='center', fontsize=11)\n",
    "    ax1.annotate('', xy=(x+0.5, 2), xytext=(x+0.5, 1.3),\n",
    "                arrowprops=dict(arrowstyle='->', color='black'))\n",
    "    # RNN box\n",
    "    rect = mpatches.FancyBboxPatch((x, 2), 1, 1, boxstyle='round,pad=0.05',\n",
    "                                    facecolor='steelblue', edgecolor='black')\n",
    "    ax1.add_patch(rect)\n",
    "    ax1.text(x+0.5, 2.5, 'RNN', ha='center', va='center', color='white', fontweight='bold')\n",
    "    # Output\n",
    "    ax1.annotate('', xy=(x+0.5, 4.5), xytext=(x+0.5, 3.2),\n",
    "                arrowprops=dict(arrowstyle='->', color='black'))\n",
    "    ax1.text(x+0.5, 4.7, f'≈∑{i+1}', ha='center', fontsize=11)\n",
    "    # Forward arrow\n",
    "    if i < 3:\n",
    "        ax1.annotate('', xy=(x+2, 2.5), xytext=(x+1.2, 2.5),\n",
    "                    arrowprops=dict(arrowstyle='->', color='steelblue', lw=2))\n",
    "\n",
    "ax1.text(5, 5.5, 'Information flows only LEFT ‚Üí RIGHT', ha='center', fontsize=11, style='italic')\n",
    "\n",
    "# Bidirectional RNN\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 6)\n",
    "ax2.set_title('Bidirectional RNN', fontsize=14, fontweight='bold')\n",
    "ax2.axis('off')\n",
    "\n",
    "for i, (x, word) in enumerate([(1, 'x‚ÇÅ'), (3.5, 'x‚ÇÇ'), (6, 'x‚ÇÉ'), (8.5, 'x‚ÇÑ')]):\n",
    "    # Input\n",
    "    ax2.text(x+0.5, 0.3, word, ha='center', fontsize=11)\n",
    "    \n",
    "    # Forward RNN (blue)\n",
    "    rect_f = mpatches.FancyBboxPatch((x, 1.2), 1, 0.8, boxstyle='round,pad=0.05',\n",
    "                                      facecolor='steelblue', edgecolor='black')\n",
    "    ax2.add_patch(rect_f)\n",
    "    ax2.text(x+0.5, 1.6, '‚Üí', ha='center', va='center', color='white', fontsize=14)\n",
    "    \n",
    "    # Backward RNN (green)\n",
    "    rect_b = mpatches.FancyBboxPatch((x, 2.2), 1, 0.8, boxstyle='round,pad=0.05',\n",
    "                                      facecolor='forestgreen', edgecolor='black')\n",
    "    ax2.add_patch(rect_b)\n",
    "    ax2.text(x+0.5, 2.6, '‚Üê', ha='center', va='center', color='white', fontsize=14)\n",
    "    \n",
    "    # Concat\n",
    "    rect_c = mpatches.FancyBboxPatch((x+0.2, 3.3), 0.6, 0.5, boxstyle='round,pad=0.02',\n",
    "                                      facecolor='orange', edgecolor='black')\n",
    "    ax2.add_patch(rect_c)\n",
    "    ax2.text(x+0.5, 3.55, 'C', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Output\n",
    "    ax2.text(x+0.5, 4.3, f'≈∑{i+1}', ha='center', fontsize=11)\n",
    "    \n",
    "    # Forward arrows\n",
    "    if i < 3:\n",
    "        ax2.annotate('', xy=(x+2, 1.6), xytext=(x+1.2, 1.6),\n",
    "                    arrowprops=dict(arrowstyle='->', color='steelblue', lw=1.5))\n",
    "    # Backward arrows\n",
    "    if i > 0:\n",
    "        ax2.annotate('', xy=(x-0.8, 2.6), xytext=(x, 2.6),\n",
    "                    arrowprops=dict(arrowstyle='->', color='forestgreen', lw=1.5))\n",
    "\n",
    "ax2.text(5, 5.2, 'Information flows BOTH directions!', ha='center', fontsize=11, style='italic')\n",
    "\n",
    "# Legend\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor='steelblue', label='Forward RNN'),\n",
    "    mpatches.Patch(facecolor='forestgreen', label='Backward RNN'),\n",
    "    mpatches.Patch(facecolor='orange', label='Concatenate')\n",
    "]\n",
    "ax2.legend(handles=legend_elements, loc='lower right', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941c3c0a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Bidirectional RNNs** use **two RNNs**: one forward (‚Üí) and one backward (‚Üê)\n",
    "\n",
    "2. **Why use them?** When future context affects current predictions:\n",
    "   - Named Entity Recognition\n",
    "   - POS Tagging\n",
    "   - Machine Translation\n",
    "\n",
    "3. **Implementation in Keras** is simple:\n",
    "   ```python\n",
    "   Bidirectional(LSTM(units))  # Just wrap with Bidirectional!\n",
    "   ```\n",
    "\n",
    "4. **Parameter count doubles** (2x weights and biases)\n",
    "\n",
    "5. **Common variants**:\n",
    "   - **BiLSTM** - Most popular in practice\n",
    "   - **BiGRU** - Faster alternative\n",
    "\n",
    "6. **Limitations**:\n",
    "   - Cannot be used for real-time applications\n",
    "   - Requires full sequence before processing\n",
    "   - Higher computational cost\n",
    "\n",
    "### Mathematical Summary\n",
    "\n",
    "| Direction | Equation |\n",
    "|-----------|----------|\n",
    "| Forward | $\\overrightarrow{h_t} = f(\\overrightarrow{h_{t-1}}, x_t)$ |\n",
    "| Backward | $\\overleftarrow{h_t} = f(\\overleftarrow{h_{t+1}}, x_t)$ |\n",
    "| Output | $\\hat{y}_t = g([\\overrightarrow{h_t}; \\overleftarrow{h_t}])$ |\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "Now that you've mastered RNNs (Vanilla, LSTM, GRU, Deep, Bidirectional), you're ready for:\n",
    "- **Encoder-Decoder Architecture** (Sequence-to-Sequence models)\n",
    "- **Attention Mechanism** (The foundation of Transformers!)\n",
    "- **Transformers** (State-of-the-art for NLP)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** üéâ You now understand Bidirectional RNNs and can apply them to various NLP tasks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
